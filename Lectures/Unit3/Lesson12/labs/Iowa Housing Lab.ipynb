{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iowa Housing Lab -- Solutions\n",
    "\n",
    "Welcome!! This lab is going to be a bit more of an advanced version of yesterday's class, where we build a regression model to predict housing prices, but this time do so with a dataset that has a more interesting mix of data -- ordinal and nominal features, as well as some missing values.\n",
    "\n",
    "**Important:** A summary of each of the columns in this dataset, and what their values mean, can be found here: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1).  Load in both your training & test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also....when you're cleaning training & test sets, it's usually a good idea to separate the column you're trying to predict from everything else.  \n",
    "\n",
    "For now, declare `y` to be the `SalePrice` column, and then remove it from the training set entirely.  You can drop the `ID` column too, since it encodes nothing meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2).  There are missing values throughout this dataset.  For the time being, let's try and do a few things:**\n",
    "\n",
    " - were these missing values likely to be randomly occurring, or are they likely encoding for something else?  \n",
    " \n",
    "If values are encoding for something else, there are usually either high correlations with missing values in similar columns, and/or they could potentially represent a particular rank in a hierarchy -- ie, 'None', 0, 'Other', etc.  Ie, the missing values basically are encoding for something specific, it's just not mentioned.\n",
    "\n",
    "Take a look at the column descriptions, see what you think they might be.\n",
    "\n",
    " - if you think they are missing at random, fill in the missing values with their mean(numeric columns) or mode(categorical columns)\n",
    " - if you think they are **not** missing at random, then go ahead and fill them in with a value to encode what they are (0, 'Other', and 'None' are common choices)\n",
    " \n",
    "**Hint:** You can try encoding null & non-null values to 0 and 1, respectively, and use the corr() method on that. \n",
    " \n",
    "*If filling in missing values, make sure to perform this operation on the training and test set, using values from the training set for imputation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3): Ordinal vs Nominal Data**\n",
    "\n",
    "There are a number of categorical columns in this dataset, and they could represent both ordinal data(data that has a rank) or nominal data (data that doesn't have a rank).  \n",
    "\n",
    "There is a file called `data_description.txt` that contains descriptions of all the values in each and what they mean if you want to do a little bit of research.\n",
    "\n",
    "You can also find a brief description here:  https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here (no real code required for this one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4):  Go Ahead and Change Your Ordinal Variables To Their Appropriate Values, if they exist.**\n",
    "\n",
    "**Hint:** The `map` method is useful for this.  \n",
    "\n",
    "It goes like this:  `mapping = {'oldColVal1': 'NewColVal1',`\n",
    "                                 `oldColVal2': 'NewColVal2', etc}`\n",
    "                                \n",
    "`df['Col'] = df['Col'].map(mapping)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5):  Now, OneHot Encode Your Dataset For Your Remaining Categorical Columns** \n",
    "\n",
    "**Note:** You want your training and your test sets attached for this one.  Detach them when you're finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6): Standardize Your Data On Your Training and Test Sets**\n",
    "\n",
    "**Remember:** Use the values from your training set to standardize your test set!  \n",
    "\n",
    "Ask me if you have any questions on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7):  Create a validation set out of your training set**\n",
    "\n",
    "Since there is no time based component, random shuffling is fine.  (You can use `train_test_split` for this, although homespun methods usually work equally as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8): Fit Linear Regression on your training set, and score it on your validation set to get a feel for how you did.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9):  Finally, go ahead and make your predictions on your test set.**\n",
    "\n",
    "Save to a csv file the following the following columns: ID of of each row in your test set, as well as your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:** Can you improve your score?\n",
    "\n",
    "The first part of this lab was meant to be a walk through of the basics of prepping a data set and getting it ready.\n",
    "\n",
    "However, there's a lot that could be improved upon!  \n",
    "\n",
    "Using validation scores as your guide, you could try and look at some of the following:\n",
    "\n",
    " - Removing outliers from the target variable, or using log transformations to make the data smoother\n",
    " - There are lots of highly correlated variables in this dataset.  Do the 4 different columns about the fireplace really tell you something that different from one another?  You can try averaging multiple columns into one if they're highly correlated, or removing some entirely to see if it improves anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
