{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: KFold, Regularization, & Pipelines\n",
    "\n",
    "Welcome!  This lab is going to introduce us to some very important aspects of data processing and model building.  \n",
    "\n",
    "Specifically, it's going to go over the following:\n",
    "\n",
    " - **KFold Cross Validation:** This is a more thorough way of choosing your validation set to give you a better idea of how your model might perform under various circumstances within your training data.\n",
    " - **Regularization:** This is an evergreen technique for dealing with models that are overfit (ie, higher scores on training vs. test data).  Regularized linear models are often much better prepared to handle messy data & outliers when using this technique.\n",
    " - **Pipelines:** (Time permitting!) This is an underappreciated aspect of the Scikit-Learn api that allows you to chain together multiple data processing steps, making it much easier to test different models and work seamlessly between your training & test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This lab builds off of the one performed in the last class.  As such, it might be easier just to keep working in your previous lab to answer these questions.  It assumes you already have your data processed from the iowa housing lab.  \n",
    "\n",
    "The questions are listed here just to make the separation of concerns easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: How Does Your Validation Score Differ Using KFold Cross Validation?\n",
    "\n",
    "Take a look at the validation score you got from your previous exercise.  \n",
    "\n",
    "This time, run your model through KFold cross validation using `cross_val_score`.  Is your total validation score appreciably different?  What were your highest and lowest values?\n",
    "\n",
    "What if you changed your number of folds?  Try using 5, 10, & 25 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the steps from the previous lab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('../data/iowa_housing/train.csv')\n",
    "test  = pd.read_csv('../data/iowa_housing/test.csv')\n",
    "\n",
    "# your answer here\n",
    "y = train['SalePrice']\n",
    "train.drop('SalePrice', axis=1, inplace=True)\n",
    "train.drop('Id', axis=1, inplace=True)\n",
    "test.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "train_empty = train.loc[:, train.isnull().sum() > 0]\n",
    "# grab the columns\n",
    "cols = train_empty.columns.tolist()\n",
    "# fill with the appropriate value  -- NA, Other, could also work\n",
    "train[['GarageType', 'GarageFinish']] = train[['GarageType', 'GarageFinish']].fillna('None')\n",
    "test[['GarageType', 'GarageFinish']]  = test[['GarageType', 'GarageFinish']].fillna('None')\n",
    "\n",
    "# we'll use this for GarageYrBlt since it's a numeric column\n",
    "train['GarageYrBlt'].fillna(0, inplace=True)\n",
    "test['GarageYrBlt'].fillna(0, inplace=True)\n",
    "\n",
    "# finding the values to use in the training set\n",
    "ms_mode   = train['MSZoning'].mode()[0]\n",
    "gcarsmean = train['GarageCars'].mean()\n",
    "\n",
    "# and applying them to the test set\n",
    "test['MSZoning'].fillna(ms_mode, inplace=True)\n",
    "test['GarageCars'].fillna(gcarsmean, inplace=True)\n",
    "\n",
    "# your code here\n",
    "# we'll assume the GarageFinish is ordinal.  Ie, FinishedGarage > Unfinished Garage\n",
    "garage_mapping = {\n",
    "    'None': 0, # no garage\n",
    "    'Unf' : 1, # unfinished garage\n",
    "    'RFn' : 2, # partially finished garage\n",
    "    'Fin' : 3  # finished garage\n",
    "}\n",
    "\n",
    "train['GarageFinish'] = train['GarageFinish'].map(garage_mapping)\n",
    "test['GarageFinish']  = test['GarageFinish'].map(garage_mapping)\n",
    "\n",
    "# MSSubClass is really a category, moreso than a true number\n",
    "# so we'll add it to the list of items to be encoded\n",
    "train['MSSubClass'] = train['MSSubClass'].astype(str)\n",
    "test['MSSubClass']  = test['MSSubClass'].astype(str)\n",
    "\n",
    "# concatenate and encode\n",
    "master = pd.concat([train, test])\n",
    "master = pd.get_dummies(master)\n",
    "\n",
    "# drop MSSubClass150\n",
    "master.drop('MSSubClass_150', axis=1, inplace=True)\n",
    "\n",
    "# and split back apart\n",
    "train  = master.iloc[:1460].copy()\n",
    "test   = master.iloc[1460:].copy()\n",
    "\n",
    "# save these values, to use on both your training and test set\n",
    "train_means = train.mean()\n",
    "train_stds  = train.std()\n",
    "\n",
    "# standardize the training set\n",
    "train -= train_means\n",
    "train /= train_stds\n",
    "\n",
    "# and do the same for the test set\n",
    "test -= train_means\n",
    "test /= train_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the part where we use KFold to find your validation score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lreg = LinearRegression()\n",
    "# we'll use a loop to go through these\n",
    "cv_scores = []\n",
    "num_folds = [5, 10, 25]\n",
    "\n",
    "for fold in num_folds:\n",
    "    scores = cross_val_score(estimator=lreg, X=train, y=np.log(y), cv=fold)\n",
    "    cv_scores.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we'll look at the different scores\n",
    "cv_dict = {}\n",
    "for idx, fold in enumerate(num_folds):\n",
    "    cv_dict[f'folds: {fold}'] = np.mean(cv_scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folds: 5': 0.8645586208190448,\n",
       " 'folds: 10': 0.8627736666706329,\n",
       " 'folds: 25': 0.8613078461662019}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores are roughly the same, but lower than just the general validation score\n",
    "# we had before\n",
    "cv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89302007, 0.82477951, 0.87092049, 0.92495534, 0.91648344,\n",
       "       0.93185865, 0.88948408, 0.84754801, 0.80089708, 0.83188453,\n",
       "       0.88281676, 0.89318777, 0.82773939, 0.8835977 , 0.84607624,\n",
       "       0.90444633, 0.82149406, 0.91933525, 0.87860388, 0.88391928,\n",
       "       0.88033665, 0.91063222, 0.49020936, 0.91349123, 0.86497884])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x209838d8278>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(y).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008402772939781673"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.log(68000) - np.log(70000))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008402772939781673"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.log(680000) - np.log(700000))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       12.247694\n",
       "1       12.109011\n",
       "2       12.317167\n",
       "3       11.849398\n",
       "4       12.429216\n",
       "5       11.870600\n",
       "6       12.634603\n",
       "7       12.206073\n",
       "8       11.774520\n",
       "9       11.678440\n",
       "10      11.771436\n",
       "11      12.751300\n",
       "12      11.877569\n",
       "13      12.540758\n",
       "14      11.964001\n",
       "15      11.790557\n",
       "16      11.911702\n",
       "17      11.407565\n",
       "18      11.976659\n",
       "19      11.842229\n",
       "20      12.692503\n",
       "21      11.845103\n",
       "22      12.345835\n",
       "23      11.774520\n",
       "24      11.944708\n",
       "25      12.454104\n",
       "26      11.811547\n",
       "27      12.631340\n",
       "28      12.242887\n",
       "29      11.134589\n",
       "          ...    \n",
       "1430    12.165980\n",
       "1431    11.875831\n",
       "1432    11.074421\n",
       "1433    12.136187\n",
       "1434    11.982929\n",
       "1435    12.066811\n",
       "1436    11.699405\n",
       "1437    12.885671\n",
       "1438    11.916389\n",
       "1439    12.190959\n",
       "1440    12.160029\n",
       "1441    11.913713\n",
       "1442    12.644328\n",
       "1443    11.703546\n",
       "1444    12.098487\n",
       "1445    11.767568\n",
       "1446    11.969717\n",
       "1447    12.388394\n",
       "1448    11.626254\n",
       "1449    11.429544\n",
       "1450    11.820410\n",
       "1451    12.567551\n",
       "1452    11.884489\n",
       "1453    11.344507\n",
       "1454    12.128111\n",
       "1455    12.072541\n",
       "1456    12.254863\n",
       "1457    12.493130\n",
       "1458    11.864462\n",
       "1459    11.901583\n",
       "Name: SalePrice, Length: 1460, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89179913, 0.82489895, 0.87746131, 0.86067351, 0.89600987,\n",
       "       0.90576855, 0.89201931, 0.81643292, 0.74565768, 0.79187226,\n",
       "       0.87308117, 0.80829943, 0.8144443 , 0.82323958, 0.90730437,\n",
       "       0.78436402, 0.8719923 , 0.84965521, 0.77001554, 0.81476842,\n",
       "       0.75186355, 0.86480539, 0.11688019, 0.86987915, 0.87815075])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Updating Your Model With Ridge & Lasso Regression\n",
    "\n",
    "Instead of using Linear Regression, import `Ridge` or `Lasso`, and use cross validation to find the ideal value of alpha.  \n",
    "\n",
    "Some basic tips:\n",
    "\n",
    "For values of alpha try this:  `alphas = np.logspace(-4, 4, 9)`\n",
    "Then write a `for-loop` that generically goes like this:\n",
    "\n",
    "`for value in alphas:\n",
    "    1). set value of alpha to current value using set_params() method\n",
    "    2). pass in instance of Ridge or Lasso into cross_val_score\n",
    "    3). using a tuple, append the average of all results from step 2 into a list, along with the value of alpha`\n",
    "    \n",
    "When you're finished, you should have a list that has 9 tuples inside it, each one with the average cross validation score as well as the value of alpha associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 699028884335.3823, tolerance: 852398977.1246126\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 686696896294.1489, tolerance: 827439703.8697118\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 698419105432.2222, tolerance: 829042909.2707292\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 633413523517.1704, tolerance: 820538852.0864891\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 631066377568.7698, tolerance: 785203388.8728524\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 674931646667.3359, tolerance: 834664899.9449159\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 678975331140.6466, tolerance: 844788544.228661\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 679337390930.1389, tolerance: 846871145.5104938\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 559162960242.36, tolerance: 801728446.7415173\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 686637436907.9114, tolerance: 843859692.9252498\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 696433272198.7897, tolerance: 852398977.1246126\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 685113214587.6981, tolerance: 827439703.8697118\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 696596752670.4814, tolerance: 829042909.2707292\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 631673551259.1697, tolerance: 820538852.0864891\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 624208830521.9236, tolerance: 785203388.8728524\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 673228704079.6665, tolerance: 834664899.9449159\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 675129508556.7335, tolerance: 844788544.228661\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 677736458263.4377, tolerance: 846871145.5104938\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 557113930385.7609, tolerance: 801728446.7415173\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 683456185437.7449, tolerance: 843859692.9252498\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 671037059100.8706, tolerance: 852398977.1246126\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 669239985244.6819, tolerance: 827439703.8697118\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 678684491275.5731, tolerance: 829042909.2707292\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 614102917569.1448, tolerance: 820538852.0864891\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 557443880125.1488, tolerance: 785203388.8728524\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 656476886337.6578, tolerance: 834664899.9449159\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 637226788597.268, tolerance: 844788544.228661\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 661583877976.0353, tolerance: 846871145.5104938\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 536849793680.3871, tolerance: 801728446.7415173\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 652352366275.5145, tolerance: 843859692.9252498\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 467539125221.6372, tolerance: 852398977.1246126\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 507634282368.25757, tolerance: 827439703.8697118\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527459574647.6899, tolerance: 829042909.2707292\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 421616030033.7689, tolerance: 820538852.0864891\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105723553214.35022, tolerance: 785203388.8728524\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 514115404416.28217, tolerance: 834664899.9449159\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 345593929396.8225, tolerance: 844788544.228661\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 486063726864.49194, tolerance: 846871145.5104938\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 356309639923.80054, tolerance: 801728446.7415173\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 410764134328.55475, tolerance: 843859692.9252498\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47424409316.1687, tolerance: 852398977.1246126\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1064088126.7036133, tolerance: 827439703.8697118\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6018953553.595459, tolerance: 820538852.0864891\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11012023568.37378, tolerance: 844788544.228661\n",
      "  positive)\n",
      "C:\\Users\\Jonat\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25966127694.608154, tolerance: 843859692.9252498\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "# your answer here\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "ridge = Ridge()\n",
    "# the max_iter argument will silence a warning message\n",
    "lasso = Lasso()\n",
    "alphas = np.logspace(-3, 3, 7)\n",
    "ridge_scores = []\n",
    "lasso_scores = []\n",
    "\n",
    "for value in alphas:\n",
    "    ridge.set_params(alpha=value)\n",
    "    lasso.set_params(alpha=value)\n",
    "    lasso_score = cross_val_score(estimator=lasso, X=train, y=y, cv=10)\n",
    "    ridge_score = cross_val_score(estimator=ridge, X=train, y=y, cv=10)\n",
    "    ridge_scores.append((np.mean(ridge_score), value))\n",
    "    lasso_scores.append((np.mean(lasso_score), value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8652092706925322, 100.0)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the value of alpha that gives the best score is 100\n",
    "max(ridge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:** In Scikit-Learn cross validation is sometimes built into algorithms automatically.  Luckily this is the case with `Ridge` and `Lasso`.  If you're inclined to take a look at the `RidgeCV` and `LassoCV` methods, you can basically combine what we just did into one step.\n",
    "\n",
    "**RidgeCV:** https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html\n",
    "**LassoCV:** https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Building A Pipeline\n",
    "\n",
    "Let's try building some pipelines to test out different versions of our models more easily.  \n",
    "\n",
    "For this one, we are going to start fresh a little bit to get the hang of using our pipelines, and to go through the entire process.\n",
    "\n",
    "So......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Reload the training and test sets\n",
    "\n",
    " - create a new variable for `y`, and set it equal to the log of `SalePrice`\n",
    " - create a variable for the `id` column in the test set -- this will be reused later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/iowa_housing/train.csv')\n",
    "test  = pd.read_csv('../data/iowa_housing/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(train['SalePrice'])\n",
    "train.drop('SalePrice', axis=1, inplace=True)\n",
    "test_id = test['Id']\n",
    "train.drop('Id', axis=1, inplace=True)\n",
    "test.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Fill in the missing data on training & test\n",
    "\n",
    "**Note:** If you feel like you have a good handle on this, you can just copy and paste from your previous solutions or the lab manual.  \n",
    "\n",
    "If you have the time and think you need extra practice, feel free to try and re-create the results on your own.....just be mindful of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_empty = train.loc[:, train.isnull().sum() > 0]\n",
    "# grab the columns\n",
    "cols = train_empty.columns.tolist()\n",
    "# fill with the appropriate value  -- NA, Other, could also work\n",
    "train[['GarageType', 'GarageFinish']] = train[['GarageType', 'GarageFinish']].fillna('None')\n",
    "test[['GarageType', 'GarageFinish']]  = test[['GarageType', 'GarageFinish']].fillna('None')\n",
    "\n",
    "# we'll use this for GarageYrBlt since it's a numeric column\n",
    "train['GarageYrBlt'].fillna(0, inplace=True)\n",
    "test['GarageYrBlt'].fillna(0, inplace=True)\n",
    "\n",
    "# finding the values to use in the training set\n",
    "ms_mode   = train['MSZoning'].mode()[0]\n",
    "gcarsmean = train['GarageCars'].mean()\n",
    "\n",
    "# and applying them to the test set\n",
    "test['MSZoning'].fillna(ms_mode, inplace=True)\n",
    "test['GarageCars'].fillna(gcarsmean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Reclassify the `MSSubClass` column as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['MSSubClass'] = train['MSSubClass'].astype(str)\n",
    "test['MSSubClass']  = test['MSSubClass'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Create Your Pipeline!\n",
    "\n",
    "\n",
    "a). Initialize instances for each of the following items:\n",
    "\n",
    " - An ordinal encoder for the `GarageFinish` column (be careful about the mapping dictionary here)\n",
    " - A categorical encoder for your nominal columns\n",
    " - The standard scaler\n",
    " - Lasso or Ridge regression, with the cross validated value of alpha from the previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# mapping for the ordinal column\n",
    "mapping = {\n",
    "    'col': 'GarageFinish',\n",
    "    'mapping': garage_mapping\n",
    "}\n",
    "\n",
    "# initialize everything\n",
    "ore   = OrdinalEncoder(cols=['GarageFinish'], mapping=[mapping])\n",
    "ohe   = OneHotEncoder()\n",
    "sc    = StandardScaler()\n",
    "ridge = Ridge(alpha=100)\n",
    "\n",
    "# make the pipeline\n",
    "pipe = make_pipeline(ore, ohe, sc, ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Fit the pipeline on your training set, and predict the values on your test set\n",
    "\n",
    " - to get the \"real\" values of your prediction you would use the function `np.exp()`\n",
    " \n",
    "ie, if `pipe.predict(test)` gives you the predicted log values of your test set, then `np.exp(pipe.predict(test))` would give you the actual expected housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('ordinalencoder',\n",
       "                 OrdinalEncoder(cols=['GarageFinish'], drop_invariant=False,\n",
       "                                handle_missing='value', handle_unknown='value',\n",
       "                                mapping=[{'col': 'GarageFinish',\n",
       "                                          'mapping': {'Fin': 3, 'None': 0,\n",
       "                                                      'RFn': 2, 'Unf': 1}}],\n",
       "                                return_df=True, verbose=0)),\n",
       "                ('onehotencoder',\n",
       "                 OneHotEncoder(cols=['MSSubClass', 'MSZoning', 'Neighborhood',\n",
       "                                     'GarageType'],\n",
       "                               drop_invariant=False, handle_missing='value',\n",
       "                               handle_unknown='value', return_df=True,\n",
       "                               use_cat_names=False, verbose=0)),\n",
       "                ('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('ridge',\n",
       "                 Ridge(alpha=100, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on the training set\n",
    "pipe.fit(train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.67852521, 11.9448299 , 12.05936728, ..., 11.92412032,\n",
       "       11.66149733, 12.3309958 ])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and predict on the test set\n",
    "pipe.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and create a dataframe from our test predictions\n",
    "df              = pd.DataFrame()\n",
    "df['Id']        = test_id\n",
    "df['SalePrice'] = np.exp(pipe.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and output to csv\n",
    "df.to_csv('submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
