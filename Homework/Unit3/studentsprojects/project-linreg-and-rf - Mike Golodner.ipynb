{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Linear Regression and Random Forests - Train/Test Split\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We've discussed overfitting in the context of bias and variance, and we've touched on some techniques, such as regularization, that are used to avoid overfitting (but haven't practiced them yet). In this lesson we'll discuss a fundamental method for avoiding overfitting that is commonly referred to as _train/test split_ validation. \n",
    "\n",
    "The idea is similar to something called \"cross-validation\" — in fact, it is a type of cross-validation — in that we split the data set into two subsets:\n",
    "* A subset on which to train our model.\n",
    "* A subset on which to test our model's predictions.\n",
    "\n",
    "This serves two useful purposes:\n",
    "* We prevent overfitting by not using all of the data.\n",
    "* We have some remaining data we can use to evaluate our model.\n",
    "\n",
    "While this may seem like a relatively simple idea, **there are some caveats** to putting it into practice. For example, if you are not careful, it is easy to take a non-random split. Suppose we have salary data on technical professionals that is composed of 80 percent data from California and 20 percent data from elsewhere and is sorted by state. If we split our data into 80 percent training data and 20 percent testing data, we might inadvertantly select all the California data to train and all the non-California data to test. In this case we've still overfit on our data set because we did not sufficiently randomize the data.\n",
    "\n",
    "In a situation like this we can use _k-fold cross-validation_, which is the same idea applied to more than two subsets. In particular, we partition our data into $k$ subsets and train on $k-1$ one of them, holding the last slice for testing. We can do this for each of the possible $k-1$ subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Practice\n",
    "\n",
    "Ultimately we use a test-training split to compare multiple models on the same data set. This could be comparisons of two linear models or of completely different models on the same data.\n",
    "\n",
    "For your independent practice, fit three different models on the Boston housing data. For example, you could pick three different subsets of variables, one or more polynomial models, or any other model you'd like. \n",
    "\n",
    "### Here's What We Will Be Doing:\n",
    "\n",
    "* Working with Boston housing data to predict the value of a home\n",
    "* Create a test-train split of the data.\n",
    "* Train each of your models on the training data.\n",
    "* Evaluate each of the models on the test data.\n",
    "* Rank the models by how well they score on the testing data set.\n",
    "\n",
    "**Then, try k-folds.**\n",
    "\n",
    "* Try a few different splits of data for the same models.\n",
    "* Perform a k-fold cross-validation and use the cross-validation scores to compare your models. Did this change your rankings?\n",
    "\n",
    "**Be sure to provide interpretation for your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that k-fold cross-validation creates a hold portion of your data set for each iteration of training and validating:\n",
    "\n",
    "![](http://i.imgur.com/0PFrPXJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Use Case\n",
    "\n",
    "In this given task, you will be asked to model the median home price of various houses across U.S. Census tracts in the city of Boston. This is a probable use case: We are predicting a continuous, numeric output (price) based on a combination of discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "X = pd.DataFrame(boston.data,\n",
    "                 columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target,\n",
    "                 columns=['MEDV'])\n",
    "\n",
    "print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean Up Data and Perform Exporatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Boston data is from scikit-learn, so it ought to be pretty clean, but we should always perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis.\n",
    "\n",
    "# Include: total nulls, index, data types, shape, summary statistics, and the number of unique values for each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  (506, 13)\n",
      "y =  (506, 1)\n"
     ]
    }
   ],
   "source": [
    "# Understand number of rows / columns in datasets\n",
    "\n",
    "print(\"X = \", X.shape)\n",
    "print(\"y = \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test whether there are nulls in dataset.\n",
    "\n",
    "np.sum(X.isnull())\n",
    "\n",
    "# Result there are no nulls in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       float64\n",
       "ZN         float64\n",
       "INDUS      float64\n",
       "CHAS       float64\n",
       "NOX        float64\n",
       "RM         float64\n",
       "AGE        float64\n",
       "DIS        float64\n",
       "RAD        float64\n",
       "TAX        float64\n",
       "PTRATIO    float64\n",
       "B          float64\n",
       "LSTAT      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the data types of each column in dataset\n",
    "\n",
    "X.dtypes\n",
    "\n",
    "# Result: All columns are numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       504\n",
       "ZN          26\n",
       "INDUS       76\n",
       "CHAS         2\n",
       "NOX         81\n",
       "RM         446\n",
       "AGE        356\n",
       "DIS        412\n",
       "RAD          9\n",
       "TAX         66\n",
       "PTRATIO     46\n",
       "B          357\n",
       "LSTAT      455\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Checking counts of unqiue values in each column. Goal to identify non-continuous variables\n",
    "\n",
    "X.nunique()\n",
    "\n",
    "# CHAS and RAD are not contiuous cariables. CHAS is a binary vairable and RAD is ordinal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `scikit-learn` Linear Regression\n",
    "\n",
    "### 2. Pick 3-4 predictors (i.e. CRIM, ZN, etc...) that you will use to predict our target variable, MEDV.\n",
    "Score and plot your predictions. What do these results tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT     -0.737663\n",
       "PTRATIO   -0.507787\n",
       "INDUS     -0.483725\n",
       "TAX       -0.468536\n",
       "NOX       -0.427321\n",
       "CRIM      -0.388305\n",
       "RAD       -0.381626\n",
       "AGE       -0.376955\n",
       "CHAS       0.175260\n",
       "DIS        0.249929\n",
       "B          0.333461\n",
       "ZN         0.360445\n",
       "RM         0.695360\n",
       "MEDV       1.000000\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing correlation of X variables with MEDV to select 3-4 predictors\n",
    "\n",
    "boston = pd.concat([X,y],axis=1)\n",
    "boston.corr()['MEDV'].sort_values()\n",
    "\n",
    "# Result: Variables with highest correlation are LSTAT, RM, PTRATIO, INDUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>RM</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>INDUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LSTAT</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RM</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>-0.391676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PTRATIO</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.383248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>INDUS</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LSTAT        RM   PTRATIO     INDUS\n",
       "LSTAT    1.000000 -0.613808  0.374044  0.603800\n",
       "RM      -0.613808  1.000000 -0.355501 -0.391676\n",
       "PTRATIO  0.374044 -0.355501  1.000000  0.383248\n",
       "INDUS    0.603800 -0.391676  0.383248  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the correlation \n",
    "\n",
    "X[['LSTAT', 'RM', 'PTRATIO', 'INDUS']].corr()\n",
    "\n",
    "# LSTAT is fairly correlated with RM and INDUS. Overall X variables are not extremely correlated and will be tested as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train=  (379, 4)\n",
      "y_Train=  (379, 1)\n",
      "X_Test=  (127, 4)\n",
      "y_Test=  (127, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create Train and Test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['LSTAT', 'RM', 'PTRATIO', 'INDUS']], y, random_state=2020)\n",
    "\n",
    "print(\"X_Train= \",X_train.shape)\n",
    "print(\"y_Train= \",y_train.shape)\n",
    "print(\"X_Test= \",X_test.shape)\n",
    "print(\"y_Test= \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6597915804955761"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize X Training and Test\n",
    "\n",
    "# Define mean and standard deviation to use on training and test set\n",
    "train_means = X_train.mean()\n",
    "train_stds  = X_train.std()\n",
    "\n",
    "# standardize the training set\n",
    "X_train_std = X_train - train_means\n",
    "X_train_std /= train_stds\n",
    "\n",
    "# and do the same for the test set\n",
    "X_test -= train_means\n",
    "X_test /= train_stds\n",
    "\n",
    "# Score and plot \n",
    "\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train_std, y_train)\n",
    "lreg.score(X_test, y_test)\n",
    "\n",
    "# Score is .6597"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Try 70/30 and 90/10 train/test splits (70% of the data for training - 30% for testing, then 90% for training - 10% for testing)\n",
    "Score and plot. How do your metrics change? What does this tell us about the size of training/testing splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6349572592261994"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing with 70% train and 30% test.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['LSTAT', 'RM', 'PTRATIO', 'INDUS']], y,test_size=.3 ,random_state=2020)\n",
    "\n",
    "# Standardize X Training and Test\n",
    "\n",
    "# Define mean and standard deviation to use on training and test set\n",
    "train_means = X_train.mean()\n",
    "train_stds  = X_train.std()\n",
    "\n",
    "# standardize the training set\n",
    "X_train_std = X_train - train_means\n",
    "X_train_std /= train_stds\n",
    "\n",
    "# and do the same for the test set\n",
    "X_test -= train_means\n",
    "X_test /= train_stds\n",
    "\n",
    "# Score and plot \n",
    "\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train_std, y_train)\n",
    "lreg.score(X_test, y_test)\n",
    "\n",
    "# Score is .6349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6603565457791117"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing with 90% train and 10% test.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['LSTAT', 'RM', 'PTRATIO', 'INDUS']], y,test_size=.1 ,random_state=2020)\n",
    "\n",
    "# Standardize X Training and Test\n",
    "\n",
    "# Define mean and standard deviation to use on training and test set\n",
    "train_means = X_train.mean()\n",
    "train_stds  = X_train.std()\n",
    "\n",
    "# standardize the training set\n",
    "X_train_std = X_train - train_means\n",
    "X_train_std /= train_stds\n",
    "\n",
    "# and do the same for the test set\n",
    "X_test -= train_means\n",
    "X_test /= train_stds\n",
    "\n",
    "# Score and plot \n",
    "\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train_std, y_train)\n",
    "lreg.score(X_test, y_test)\n",
    "\n",
    "# Score is .6603\n",
    "# Thus score improved by decreasing the size of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use k-fold cross validation varying the number of folds from 5 to 10\n",
    "What seems optimal? How do your scores change? What is the variance like? Try different folds to get a sense of how this impacts your score. What are the tradeoffs associated with choosing the number of folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.66483586, 0.60359389, 0.48669311, 0.73382352, 0.79289127]),\n",
       " array([0.66134375, 0.66713359, 0.56387986, 0.61393163, 0.65464383,\n",
       "        0.34985327, 0.73871658, 0.71765115, 0.82363217, 0.77947662])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use KFold to find your validation score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# we'll use a loop to go through these\n",
    "cv_scores = []\n",
    "num_folds = [5, 10]\n",
    "\n",
    "for fold in num_folds:\n",
    "    scores = cross_val_score(estimator=lreg, X=X_train_std, y=y_train, cv=fold)\n",
    "    cv_scores.append(scores)\n",
    "    \n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Folds Mean:  0.6563675284480988\n",
      "5 Folds Min:  0.4866931086815816\n",
      "10 Folds Mean:  0.6570262460120486\n",
      "10 Folds Min:  0.34985327120484233\n"
     ]
    }
   ],
   "source": [
    "print(\"5 Folds Mean: \",np.mean(cv_scores[0]))\n",
    "print(\"5 Folds Min: \",np.min(cv_scores[0]))\n",
    "print(\"10 Folds Mean: \",np.mean(cv_scores[1]))\n",
    "print(\"10 Folds Min: \",np.min(cv_scores[1]))\n",
    "\n",
    "# While 10 fold has a slightly higher score it has a larger range, thus 5 folds seems more optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ##  Mike adding section to test with Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>-0.435910</td>\n",
       "      <td>1.782441</td>\n",
       "      <td>-0.833462</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.291008</td>\n",
       "      <td>0.393812</td>\n",
       "      <td>-1.625233</td>\n",
       "      <td>1.674581</td>\n",
       "      <td>-0.405469</td>\n",
       "      <td>-0.663828</td>\n",
       "      <td>-0.837170</td>\n",
       "      <td>0.151275</td>\n",
       "      <td>-0.437670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>-0.411232</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.363418</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.293324</td>\n",
       "      <td>0.169819</td>\n",
       "      <td>0.602560</td>\n",
       "      <td>-0.514753</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.128262</td>\n",
       "      <td>1.137183</td>\n",
       "      <td>-3.166141</td>\n",
       "      <td>-0.278134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>-0.421579</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>2.419124</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.478797</td>\n",
       "      <td>-1.161538</td>\n",
       "      <td>0.862766</td>\n",
       "      <td>-0.939919</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>1.796246</td>\n",
       "      <td>0.769861</td>\n",
       "      <td>0.408911</td>\n",
       "      <td>0.780210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>-0.334996</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.424350</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.137165</td>\n",
       "      <td>-1.158738</td>\n",
       "      <td>-1.136901</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.581433</td>\n",
       "      <td>1.183098</td>\n",
       "      <td>-0.760761</td>\n",
       "      <td>-0.127146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>-0.242865</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.235310</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>2.751781</td>\n",
       "      <td>-1.231536</td>\n",
       "      <td>1.122972</td>\n",
       "      <td>-1.048993</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.016440</td>\n",
       "      <td>-1.709558</td>\n",
       "      <td>-0.180776</td>\n",
       "      <td>0.099337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "289 -0.435910  1.782441 -0.833462 -0.274743 -1.291008  0.393812 -1.625233   \n",
       "102 -0.411232 -0.483615 -0.363418 -0.274743 -0.293324  0.169819  0.602560   \n",
       "488 -0.421579 -0.483615  2.419124 -0.274743  0.478797 -1.161538  0.862766   \n",
       "18  -0.334996 -0.483615 -0.424350 -0.274743 -0.137165 -1.158738 -1.136901   \n",
       "151 -0.242865 -0.483615  1.235310 -0.274743  2.751781 -1.231536  1.122972   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "289  1.674581 -0.405469 -0.663828 -0.837170  0.151275 -0.437670  \n",
       "102 -0.514753 -0.520559 -0.128262  1.137183 -3.166141 -0.278134  \n",
       "488 -0.939919 -0.635649  1.796246  0.769861  0.408911  0.780210  \n",
       "18  -0.000158 -0.635649 -0.581433  1.183098 -0.760761 -0.127146  \n",
       "151 -1.048993 -0.520559 -0.016440 -1.709558 -0.180776  0.099337  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuilding train and test set to incorporate all variables\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.1 ,random_state=2020)\n",
    "\n",
    "# Standardize X Training and Test\n",
    "\n",
    "# Define mean and standard deviation to use on training and test set\n",
    "train_means = X_train.mean()\n",
    "train_stds  = X_train.std()\n",
    "\n",
    "# standardize the training set\n",
    "X_train_std = X_train - train_means\n",
    "X_train_std /= train_stds\n",
    "\n",
    "# and do the same for the test set\n",
    "X_test -= train_means\n",
    "X_test /= train_stds\n",
    "\n",
    "# Score and plot \n",
    "\n",
    "X_train_std.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7581165420849134, 1.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Ridge Regression\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "ridge = Ridge()\n",
    "\n",
    "# Test various Alpha levels to determine which returns optimal score\n",
    "alphas = np.logspace(-3,3,7)\n",
    "\n",
    "cv_scores=[]\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge.set_params(alpha=alpha)\n",
    "    scores = cross_val_score(estimator=ridge, X=X_train_std, y=np.log(y_train), cv=10)\n",
    "    cv_scores.append((np.mean(scores),alpha))\n",
    "    \n",
    "max(cv_scores)\n",
    "\n",
    "# Result: 1 returns highest score (78.8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictor</th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CRIM</td>\n",
       "      <td>-0.084838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ZN</td>\n",
       "      <td>0.026631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>INDUS</td>\n",
       "      <td>0.009928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CHAS</td>\n",
       "      <td>0.024862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NOX</td>\n",
       "      <td>-0.081605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>RM</td>\n",
       "      <td>0.062723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>AGE</td>\n",
       "      <td>0.002890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>DIS</td>\n",
       "      <td>-0.101941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>RAD</td>\n",
       "      <td>0.124369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>TAX</td>\n",
       "      <td>-0.098922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>PTRATIO</td>\n",
       "      <td>-0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>B</td>\n",
       "      <td>0.040752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>LSTAT</td>\n",
       "      <td>-0.200242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predictor  Coefficients\n",
       "0       CRIM     -0.084838\n",
       "1         ZN      0.026631\n",
       "2      INDUS      0.009928\n",
       "3       CHAS      0.024862\n",
       "4        NOX     -0.081605\n",
       "5         RM      0.062723\n",
       "6        AGE      0.002890\n",
       "7        DIS     -0.101941\n",
       "8        RAD      0.124369\n",
       "9        TAX     -0.098922\n",
       "10   PTRATIO     -0.084800\n",
       "11         B      0.040752\n",
       "12     LSTAT     -0.200242"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Ridge with Alpha of 1.0 and review results\n",
    "\n",
    "rreg = Ridge(alpha=1.0)\n",
    "rreg.fit(X_train_std, np.log(y_train))\n",
    "\n",
    "coefs = rreg.coef_\n",
    "\n",
    "coefs_list = coefs[0]\n",
    "\n",
    "col_list = X_train_std.columns.to_list()\n",
    "\n",
    "coef_dict = {'Predictor':col_list, 'Coefficients':coefs_list }\n",
    "\n",
    "coef_df = pd.DataFrame(coef_dict)\n",
    "coef_df\n",
    "\n",
    "# Result: This model most heavily weights LSTAT, RAD, DIS, Tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8087772457560763"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rreg.score(X_test, np.log(y_test))\n",
    "\n",
    "# This appraoach results in score of 80.88% which is higher than normal linear regression (66%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>MEDV_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>1.476182</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.374691</td>\n",
       "      <td>0.795599</td>\n",
       "      <td>1.122972</td>\n",
       "      <td>-1.108977</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>-1.969349</td>\n",
       "      <td>1.025210</td>\n",
       "      <td>27.5</td>\n",
       "      <td>15.620402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>-0.415507</td>\n",
       "      <td>0.465970</td>\n",
       "      <td>-0.755121</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.065444</td>\n",
       "      <td>-0.080773</td>\n",
       "      <td>0.381563</td>\n",
       "      <td>2.025778</td>\n",
       "      <td>-0.290380</td>\n",
       "      <td>-0.446070</td>\n",
       "      <td>0.310709</td>\n",
       "      <td>0.200002</td>\n",
       "      <td>-0.346507</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.404778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0.875570</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.207540</td>\n",
       "      <td>-0.604356</td>\n",
       "      <td>0.331660</td>\n",
       "      <td>-1.092376</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>-0.218699</td>\n",
       "      <td>2.476694</td>\n",
       "      <td>6.3</td>\n",
       "      <td>11.251188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>-0.435750</td>\n",
       "      <td>2.537793</td>\n",
       "      <td>-1.280293</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.334385</td>\n",
       "      <td>0.822198</td>\n",
       "      <td>-0.751938</td>\n",
       "      <td>1.917465</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.281281</td>\n",
       "      <td>-1.663643</td>\n",
       "      <td>0.362279</td>\n",
       "      <td>-0.927670</td>\n",
       "      <td>24.8</td>\n",
       "      <td>29.942523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>-0.417497</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.534607</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.527563</td>\n",
       "      <td>0.129220</td>\n",
       "      <td>-0.505990</td>\n",
       "      <td>0.353703</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.699140</td>\n",
       "      <td>0.540285</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>-0.813717</td>\n",
       "      <td>23.1</td>\n",
       "      <td>24.774775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>-0.261674</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.424350</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.137165</td>\n",
       "      <td>-0.296366</td>\n",
       "      <td>1.122972</td>\n",
       "      <td>0.179888</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.581433</td>\n",
       "      <td>1.183098</td>\n",
       "      <td>0.206506</td>\n",
       "      <td>0.065151</td>\n",
       "      <td>14.5</td>\n",
       "      <td>18.102665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>-0.438299</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-1.331070</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.310675</td>\n",
       "      <td>0.358813</td>\n",
       "      <td>-0.313509</td>\n",
       "      <td>1.174970</td>\n",
       "      <td>-0.980917</td>\n",
       "      <td>0.095381</td>\n",
       "      <td>-1.158576</td>\n",
       "      <td>0.352357</td>\n",
       "      <td>-0.560170</td>\n",
       "      <td>16.5</td>\n",
       "      <td>21.575208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>2.163005</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.016678</td>\n",
       "      <td>1.440978</td>\n",
       "      <td>1.048118</td>\n",
       "      <td>-1.179949</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>0.122128</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.375041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>1.931567</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.016678</td>\n",
       "      <td>-0.084973</td>\n",
       "      <td>1.122972</td>\n",
       "      <td>-1.146746</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.394029</td>\n",
       "      <td>1.310094</td>\n",
       "      <td>10.2</td>\n",
       "      <td>14.089632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>-0.435521</td>\n",
       "      <td>1.782441</td>\n",
       "      <td>-0.833462</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.291008</td>\n",
       "      <td>0.043823</td>\n",
       "      <td>-0.816099</td>\n",
       "      <td>1.674581</td>\n",
       "      <td>-0.405469</td>\n",
       "      <td>-0.663828</td>\n",
       "      <td>-0.837170</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>-0.709734</td>\n",
       "      <td>22.3</td>\n",
       "      <td>27.029747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>-0.382937</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.705796</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.432132</td>\n",
       "      <td>0.375613</td>\n",
       "      <td>-1.678700</td>\n",
       "      <td>-0.200612</td>\n",
       "      <td>-0.175290</td>\n",
       "      <td>-0.581433</td>\n",
       "      <td>-0.469848</td>\n",
       "      <td>0.246304</td>\n",
       "      <td>-1.256711</td>\n",
       "      <td>31.5</td>\n",
       "      <td>31.950960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>-0.324563</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.570434</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.608929</td>\n",
       "      <td>-0.905346</td>\n",
       "      <td>0.934055</td>\n",
       "      <td>-0.864285</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>0.183662</td>\n",
       "      <td>1.274928</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>0.820094</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.915996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>-0.423570</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.068915</td>\n",
       "      <td>3.631762</td>\n",
       "      <td>-0.562265</td>\n",
       "      <td>-0.307566</td>\n",
       "      <td>-0.334895</td>\n",
       "      <td>0.210427</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.757993</td>\n",
       "      <td>0.081134</td>\n",
       "      <td>0.257107</td>\n",
       "      <td>0.295907</td>\n",
       "      <td>24.4</td>\n",
       "      <td>22.284157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>-0.198043</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.235310</td>\n",
       "      <td>3.631762</td>\n",
       "      <td>0.444095</td>\n",
       "      <td>2.125556</td>\n",
       "      <td>1.058812</td>\n",
       "      <td>-0.835364</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.016440</td>\n",
       "      <td>-1.709558</td>\n",
       "      <td>0.348498</td>\n",
       "      <td>-1.518805</td>\n",
       "      <td>50.0</td>\n",
       "      <td>42.044609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>0.536935</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.086082</td>\n",
       "      <td>-0.126971</td>\n",
       "      <td>0.342353</td>\n",
       "      <td>-0.885358</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>-2.880282</td>\n",
       "      <td>1.273059</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.470982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>-0.427281</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.363418</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.293324</td>\n",
       "      <td>-0.124171</td>\n",
       "      <td>-0.502425</td>\n",
       "      <td>-0.484737</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.128262</td>\n",
       "      <td>1.137183</td>\n",
       "      <td>0.391272</td>\n",
       "      <td>0.059453</td>\n",
       "      <td>21.7</td>\n",
       "      <td>19.675269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>0.798841</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.086082</td>\n",
       "      <td>0.134820</td>\n",
       "      <td>0.966135</td>\n",
       "      <td>-0.869851</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>-3.277265</td>\n",
       "      <td>1.637711</td>\n",
       "      <td>9.5</td>\n",
       "      <td>12.252919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>-0.392577</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.169017</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.085112</td>\n",
       "      <td>0.396612</td>\n",
       "      <td>0.670285</td>\n",
       "      <td>-0.092536</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.599089</td>\n",
       "      <td>-0.010697</td>\n",
       "      <td>0.415526</td>\n",
       "      <td>-0.470431</td>\n",
       "      <td>23.8</td>\n",
       "      <td>24.388828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>-0.434945</td>\n",
       "      <td>1.027089</td>\n",
       "      <td>-0.726106</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.005583</td>\n",
       "      <td>-0.808749</td>\n",
       "      <td>-1.429187</td>\n",
       "      <td>1.352781</td>\n",
       "      <td>-0.980917</td>\n",
       "      <td>-0.599089</td>\n",
       "      <td>-0.699424</td>\n",
       "      <td>0.397115</td>\n",
       "      <td>-0.021739</td>\n",
       "      <td>17.1</td>\n",
       "      <td>20.153791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>-0.067260</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.189218</td>\n",
       "      <td>-0.730352</td>\n",
       "      <td>-1.005016</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.384989</td>\n",
       "      <td>-0.308047</td>\n",
       "      <td>21.8</td>\n",
       "      <td>21.304634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>-0.367529</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.570434</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.608929</td>\n",
       "      <td>0.071822</td>\n",
       "      <td>1.058812</td>\n",
       "      <td>-0.802066</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>0.183662</td>\n",
       "      <td>1.274928</td>\n",
       "      <td>0.404281</td>\n",
       "      <td>0.623524</td>\n",
       "      <td>18.1</td>\n",
       "      <td>16.440503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>-0.437947</td>\n",
       "      <td>2.753608</td>\n",
       "      <td>-1.177290</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.091471</td>\n",
       "      <td>0.435811</td>\n",
       "      <td>-1.664442</td>\n",
       "      <td>0.763123</td>\n",
       "      <td>-0.750738</td>\n",
       "      <td>-0.905127</td>\n",
       "      <td>-0.056612</td>\n",
       "      <td>0.414864</td>\n",
       "      <td>-1.176944</td>\n",
       "      <td>30.8</td>\n",
       "      <td>30.755921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>-0.399342</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.169017</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.085112</td>\n",
       "      <td>-0.517559</td>\n",
       "      <td>0.524141</td>\n",
       "      <td>0.095977</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.599089</td>\n",
       "      <td>-0.010697</td>\n",
       "      <td>0.360515</td>\n",
       "      <td>0.818669</td>\n",
       "      <td>17.8</td>\n",
       "      <td>17.415248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>-0.436476</td>\n",
       "      <td>1.782441</td>\n",
       "      <td>-0.833462</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.291008</td>\n",
       "      <td>-0.104572</td>\n",
       "      <td>-1.325818</td>\n",
       "      <td>1.674581</td>\n",
       "      <td>-0.405469</td>\n",
       "      <td>-0.663828</td>\n",
       "      <td>-0.837170</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>-0.775257</td>\n",
       "      <td>23.2</td>\n",
       "      <td>27.095258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>4.652545</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.207540</td>\n",
       "      <td>-1.162938</td>\n",
       "      <td>1.122972</td>\n",
       "      <td>-1.097513</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>2.565008</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.976684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>1.462288</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.268268</td>\n",
       "      <td>-1.965112</td>\n",
       "      <td>1.122972</td>\n",
       "      <td>-1.049992</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.164504</td>\n",
       "      <td>2.569282</td>\n",
       "      <td>10.2</td>\n",
       "      <td>9.732137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>-0.431129</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>0.256053</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.013391</td>\n",
       "      <td>-0.014975</td>\n",
       "      <td>-2.227628</td>\n",
       "      <td>0.216278</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.045867</td>\n",
       "      <td>0.127049</td>\n",
       "      <td>0.407037</td>\n",
       "      <td>-0.826536</td>\n",
       "      <td>24.1</td>\n",
       "      <td>25.293943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>1.406461</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.381050</td>\n",
       "      <td>-0.105972</td>\n",
       "      <td>0.944749</td>\n",
       "      <td>-0.749027</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>-2.837287</td>\n",
       "      <td>0.368552</td>\n",
       "      <td>11.7</td>\n",
       "      <td>14.508443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>-0.373632</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.705796</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.406105</td>\n",
       "      <td>1.503976</td>\n",
       "      <td>0.110663</td>\n",
       "      <td>0.167044</td>\n",
       "      <td>-0.175290</td>\n",
       "      <td>-0.581433</td>\n",
       "      <td>-0.469848</td>\n",
       "      <td>0.353570</td>\n",
       "      <td>-1.118542</td>\n",
       "      <td>31.5</td>\n",
       "      <td>32.344997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>11.376843</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.016678</td>\n",
       "      <td>0.957994</td>\n",
       "      <td>0.834250</td>\n",
       "      <td>-1.132286</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>0.659134</td>\n",
       "      <td>10.4</td>\n",
       "      <td>7.682925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>-0.428223</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.153059</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.059085</td>\n",
       "      <td>0.603805</td>\n",
       "      <td>0.467110</td>\n",
       "      <td>-0.532448</td>\n",
       "      <td>-0.405469</td>\n",
       "      <td>0.154235</td>\n",
       "      <td>-0.286188</td>\n",
       "      <td>0.414423</td>\n",
       "      <td>-0.345082</td>\n",
       "      <td>22.8</td>\n",
       "      <td>24.649605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>-0.438239</td>\n",
       "      <td>1.890348</td>\n",
       "      <td>-1.056878</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.605642</td>\n",
       "      <td>0.577206</td>\n",
       "      <td>-0.431136</td>\n",
       "      <td>0.920575</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.210656</td>\n",
       "      <td>-0.378018</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>-0.769560</td>\n",
       "      <td>23.9</td>\n",
       "      <td>26.238518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>5.634709</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.207540</td>\n",
       "      <td>-2.470496</td>\n",
       "      <td>1.122972</td>\n",
       "      <td>-1.017313</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>-2.973547</td>\n",
       "      <td>3.475213</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.865122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>-0.411108</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.068915</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.562265</td>\n",
       "      <td>0.059223</td>\n",
       "      <td>-0.570150</td>\n",
       "      <td>0.265463</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.757993</td>\n",
       "      <td>0.081134</td>\n",
       "      <td>0.406486</td>\n",
       "      <td>-0.229704</td>\n",
       "      <td>24.4</td>\n",
       "      <td>22.959860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>-0.405969</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.199483</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.270584</td>\n",
       "      <td>-0.685553</td>\n",
       "      <td>0.075018</td>\n",
       "      <td>-0.430081</td>\n",
       "      <td>-0.405469</td>\n",
       "      <td>-0.087064</td>\n",
       "      <td>0.356625</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>0.216139</td>\n",
       "      <td>18.3</td>\n",
       "      <td>18.927879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>-0.291977</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.235310</td>\n",
       "      <td>3.631762</td>\n",
       "      <td>2.751781</td>\n",
       "      <td>-1.780318</td>\n",
       "      <td>0.695236</td>\n",
       "      <td>-1.040146</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.016440</td>\n",
       "      <td>-1.709558</td>\n",
       "      <td>-0.162255</td>\n",
       "      <td>-0.065896</td>\n",
       "      <td>15.3</td>\n",
       "      <td>20.521441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>-0.053231</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.235310</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.444095</td>\n",
       "      <td>-0.255767</td>\n",
       "      <td>0.873459</td>\n",
       "      <td>-0.719915</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.016440</td>\n",
       "      <td>-1.709558</td>\n",
       "      <td>-1.299075</td>\n",
       "      <td>-0.394937</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.937962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>-0.433367</td>\n",
       "      <td>1.242904</td>\n",
       "      <td>-1.423918</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.082795</td>\n",
       "      <td>0.288815</td>\n",
       "      <td>-0.858872</td>\n",
       "      <td>2.376167</td>\n",
       "      <td>-0.980917</td>\n",
       "      <td>-0.416644</td>\n",
       "      <td>0.586200</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>-0.940490</td>\n",
       "      <td>22.9</td>\n",
       "      <td>20.754055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.161673</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.875554</td>\n",
       "      <td>0.160019</td>\n",
       "      <td>0.695236</td>\n",
       "      <td>-0.608225</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>0.182584</td>\n",
       "      <td>-0.682670</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21.653252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>-0.227391</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.424350</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.137165</td>\n",
       "      <td>-0.262767</td>\n",
       "      <td>1.012473</td>\n",
       "      <td>-0.017616</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.581433</td>\n",
       "      <td>1.183098</td>\n",
       "      <td>-1.209227</td>\n",
       "      <td>1.104978</td>\n",
       "      <td>13.5</td>\n",
       "      <td>14.142180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>-0.427061</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>0.125485</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.166478</td>\n",
       "      <td>0.714402</td>\n",
       "      <td>0.741574</td>\n",
       "      <td>-0.669731</td>\n",
       "      <td>-0.980917</td>\n",
       "      <td>-0.781535</td>\n",
       "      <td>1.183098</td>\n",
       "      <td>0.390831</td>\n",
       "      <td>-0.869269</td>\n",
       "      <td>22.0</td>\n",
       "      <td>24.807686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>-0.428675</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.740614</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.475510</td>\n",
       "      <td>-0.619756</td>\n",
       "      <td>-0.252913</td>\n",
       "      <td>-0.199280</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.746223</td>\n",
       "      <td>0.356625</td>\n",
       "      <td>0.215656</td>\n",
       "      <td>-0.167030</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.123834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>-0.257254</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.424350</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-0.137165</td>\n",
       "      <td>-0.467160</td>\n",
       "      <td>0.481368</td>\n",
       "      <td>0.091886</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.581433</td>\n",
       "      <td>1.183098</td>\n",
       "      <td>-1.382418</td>\n",
       "      <td>2.154775</td>\n",
       "      <td>13.2</td>\n",
       "      <td>11.121791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>-0.431143</td>\n",
       "      <td>2.969423</td>\n",
       "      <td>-0.887140</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.238954</td>\n",
       "      <td>1.209986</td>\n",
       "      <td>-1.454139</td>\n",
       "      <td>0.627839</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.946324</td>\n",
       "      <td>0.356625</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>-1.285200</td>\n",
       "      <td>37.3</td>\n",
       "      <td>33.606538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>-0.252336</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.235310</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>2.751781</td>\n",
       "      <td>0.316814</td>\n",
       "      <td>1.122972</td>\n",
       "      <td>-0.966082</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.016440</td>\n",
       "      <td>-1.709558</td>\n",
       "      <td>0.069585</td>\n",
       "      <td>-0.739647</td>\n",
       "      <td>23.3</td>\n",
       "      <td>24.302258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>-0.398391</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.570434</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>0.608929</td>\n",
       "      <td>0.206218</td>\n",
       "      <td>1.080198</td>\n",
       "      <td>-0.943915</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>0.183662</td>\n",
       "      <td>1.274928</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>0.399890</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.656444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>-0.439007</td>\n",
       "      <td>2.969423</td>\n",
       "      <td>-1.349929</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.464518</td>\n",
       "      <td>-0.075173</td>\n",
       "      <td>-1.318689</td>\n",
       "      <td>2.517492</td>\n",
       "      <td>-0.980917</td>\n",
       "      <td>-0.969866</td>\n",
       "      <td>-0.102527</td>\n",
       "      <td>-0.180776</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>20.1</td>\n",
       "      <td>19.347626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>1.040788</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>1.020599</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>1.615289</td>\n",
       "      <td>0.483409</td>\n",
       "      <td>0.930491</td>\n",
       "      <td>-0.795406</td>\n",
       "      <td>1.666143</td>\n",
       "      <td>1.531405</td>\n",
       "      <td>0.815776</td>\n",
       "      <td>-2.735644</td>\n",
       "      <td>1.522333</td>\n",
       "      <td>13.4</td>\n",
       "      <td>12.199567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>-0.429445</td>\n",
       "      <td>-0.483615</td>\n",
       "      <td>-0.036999</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.221603</td>\n",
       "      <td>-0.306166</td>\n",
       "      <td>-2.163468</td>\n",
       "      <td>0.708991</td>\n",
       "      <td>-0.635649</td>\n",
       "      <td>-0.593204</td>\n",
       "      <td>0.356625</td>\n",
       "      <td>0.362830</td>\n",
       "      <td>-1.006013</td>\n",
       "      <td>22.8</td>\n",
       "      <td>25.297267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>-0.429573</td>\n",
       "      <td>1.458719</td>\n",
       "      <td>-1.106203</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.013391</td>\n",
       "      <td>0.934195</td>\n",
       "      <td>-1.675136</td>\n",
       "      <td>1.276243</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.045867</td>\n",
       "      <td>-1.479982</td>\n",
       "      <td>0.216979</td>\n",
       "      <td>-1.065839</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.909028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>-0.424909</td>\n",
       "      <td>1.458719</td>\n",
       "      <td>-1.106203</td>\n",
       "      <td>-0.274743</td>\n",
       "      <td>-1.013391</td>\n",
       "      <td>0.381212</td>\n",
       "      <td>-1.404236</td>\n",
       "      <td>0.366213</td>\n",
       "      <td>-0.520559</td>\n",
       "      <td>-0.045867</td>\n",
       "      <td>-1.479982</td>\n",
       "      <td>0.273864</td>\n",
       "      <td>-1.142758</td>\n",
       "      <td>29.8</td>\n",
       "      <td>32.276933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "409   1.476182 -0.483615  1.020599 -0.274743  0.374691  0.795599  1.122972   \n",
       "247  -0.415507  0.465970 -0.755121 -0.274743 -1.065444 -0.080773  0.381563   \n",
       "399   0.875570 -0.483615  1.020599 -0.274743  1.207540 -0.604356  0.331660   \n",
       "300  -0.435750  2.537793 -1.280293 -0.274743 -1.334385  0.822198 -0.751938   \n",
       "321  -0.417497 -0.483615 -0.534607 -0.274743 -0.527563  0.129220 -0.505990   \n",
       "31   -0.261674 -0.483615 -0.424350 -0.274743 -0.137165 -0.296366  1.122972   \n",
       "342  -0.438299 -0.483615 -1.331070 -0.274743 -0.310675  0.358813 -0.313509   \n",
       "375   2.163005 -0.483615  1.020599 -0.274743  1.016678  1.440978  1.048118   \n",
       "379   1.931567 -0.483615  1.020599 -0.274743  1.016678 -0.084973  1.122972   \n",
       "288  -0.435521  1.782441 -0.833462 -0.274743 -1.291008  0.043823 -0.816099   \n",
       "229  -0.382937 -0.483615 -0.705796 -0.274743 -0.432132  0.375613 -1.678700   \n",
       "129  -0.324563 -0.483615  1.570434 -0.274743  0.608929 -0.905346  0.934055   \n",
       "208  -0.423570 -0.483615 -0.068915  3.631762 -0.562265 -0.307566 -0.334895   \n",
       "162  -0.198043 -0.483615  1.235310  3.631762  0.444095  2.125556  1.058812   \n",
       "428   0.536935 -0.483615  1.020599 -0.274743  1.086082 -0.126971  0.342353   \n",
       "110  -0.427281 -0.483615 -0.363418 -0.274743 -0.293324 -0.124171 -0.502425   \n",
       "429   0.798841 -0.483615  1.020599 -0.274743  1.086082  0.134820  0.966135   \n",
       "314  -0.392577 -0.483615 -0.169017 -0.274743 -0.085112  0.396612  0.670285   \n",
       "331  -0.434945  1.027089 -0.726106 -0.274743 -1.005583 -0.808749 -1.429187   \n",
       "483  -0.067260 -0.483615  1.020599 -0.274743 -0.189218 -0.730352 -1.005016   \n",
       "135  -0.367529 -0.483615  1.570434 -0.274743  0.608929  0.071822  1.058812   \n",
       "39   -0.437947  2.753608 -1.177290 -0.274743 -1.091471  0.435811 -1.664442   \n",
       "316  -0.399342 -0.483615 -0.169017 -0.274743 -0.085112 -0.517559  0.524141   \n",
       "287  -0.436476  1.782441 -0.833462 -0.274743 -1.291008 -0.104572 -1.325818   \n",
       "398   4.652545 -0.483615  1.020599 -0.274743  1.207540 -1.162938  1.122972   \n",
       "388   1.462288 -0.483615  1.020599 -0.274743  1.268268 -1.965112  1.122972   \n",
       "74   -0.431129 -0.483615  0.256053 -0.274743 -1.013391 -0.014975 -2.227628   \n",
       "434   1.406461 -0.483615  1.020599 -0.274743  1.381050 -0.105972  0.944749   \n",
       "237  -0.373632 -0.483615 -0.705796 -0.274743 -0.406105  1.503976  0.110663   \n",
       "380  11.376843 -0.483615  1.020599 -0.274743  1.016678  0.957994  0.834250   \n",
       "111  -0.428223 -0.483615 -0.153059 -0.274743 -0.059085  0.603805  0.467110   \n",
       "343  -0.438239  1.890348 -1.056878 -0.274743 -0.605642  0.577206 -0.431136   \n",
       "414   5.634709 -0.483615  1.020599 -0.274743  1.207540 -2.470496  1.122972   \n",
       "206  -0.411108 -0.483615 -0.068915 -0.274743 -0.562265  0.059223 -0.570150   \n",
       "497  -0.405969 -0.483615 -0.199483 -0.274743  0.270584 -0.685553  0.075018   \n",
       "152  -0.291977 -0.483615  1.235310  3.631762  2.751781 -1.780318  0.695236   \n",
       "165  -0.053231 -0.483615  1.235310 -0.274743  0.444095 -0.255767  0.873459   \n",
       "350  -0.433367  1.242904 -1.423918 -0.274743 -1.082795  0.288815 -0.858872   \n",
       "360   0.161673 -0.483615  1.020599 -0.274743  1.875554  0.160019  0.695236   \n",
       "34   -0.227391 -0.483615 -0.424350 -0.274743 -0.137165 -0.262767  1.012473   \n",
       "504  -0.427061 -0.483615  0.125485 -0.274743  0.166478  0.714402  0.741574   \n",
       "36   -0.428675 -0.483615 -0.740614 -0.274743 -0.475510 -0.619756 -0.252913   \n",
       "32   -0.257254 -0.483615 -0.424350 -0.274743 -0.137165 -0.467160  0.481368   \n",
       "291  -0.431143  2.969423 -0.887140 -0.274743 -1.238954  1.209986 -1.454139   \n",
       "159  -0.252336 -0.483615  1.235310 -0.274743  2.751781  0.316814  1.122972   \n",
       "128  -0.398391 -0.483615  1.570434 -0.274743  0.608929  0.206218  1.080198   \n",
       "286  -0.439007  2.969423 -1.349929 -0.274743 -1.464518 -0.075173 -1.318689   \n",
       "435   1.040788 -0.483615  1.020599 -0.274743  1.615289  0.483409  0.930491   \n",
       "72   -0.429445 -0.483615 -0.036999 -0.274743 -1.221603 -0.306166 -2.163468   \n",
       "190  -0.429573  1.458719 -1.106203 -0.274743 -1.013391  0.934195 -1.675136   \n",
       "188  -0.424909  1.458719 -1.106203 -0.274743 -1.013391  0.381212 -1.404236   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  MEDV  \\\n",
       "409 -1.108977  1.666143  1.531405  0.815776 -1.969349  1.025210  27.5   \n",
       "247  2.025778 -0.290380 -0.446070  0.310709  0.200002 -0.346507  20.5   \n",
       "399 -1.092376  1.666143  1.531405  0.815776 -0.218699  2.476694   6.3   \n",
       "300  1.917465 -0.520559 -0.281281 -1.663643  0.362279 -0.927670  24.8   \n",
       "321  0.353703 -0.520559 -0.699140  0.540285  0.428865 -0.813717  23.1   \n",
       "31   0.179888 -0.635649 -0.581433  1.183098  0.206506  0.065151  14.5   \n",
       "342  1.174970 -0.980917  0.095381 -1.158576  0.352357 -0.560170  16.5   \n",
       "375 -1.179949  1.666143  1.531405  0.815776  0.428865  0.122128  15.0   \n",
       "379 -1.146746  1.666143  1.531405  0.815776  0.394029  1.310094  10.2   \n",
       "288  1.674581 -0.405469 -0.663828 -0.837170  0.428865 -0.709734  22.3   \n",
       "229 -0.200612 -0.175290 -0.581433 -0.469848  0.246304 -1.256711  31.5   \n",
       "129 -0.864285 -0.635649  0.183662  1.274928  0.428865  0.820094  14.3   \n",
       "208  0.210427 -0.635649 -0.757993  0.081134  0.257107  0.295907  24.4   \n",
       "162 -0.835364 -0.520559 -0.016440 -1.709558  0.348498 -1.518805  50.0   \n",
       "428 -0.885358  1.666143  1.531405  0.815776 -2.880282  1.273059  11.0   \n",
       "110 -0.484737 -0.520559 -0.128262  1.137183  0.391272  0.059453  21.7   \n",
       "429 -0.869851  1.666143  1.531405  0.815776 -3.277265  1.637711   9.5   \n",
       "314 -0.092536 -0.635649 -0.599089 -0.010697  0.415526 -0.470431  23.8   \n",
       "331  1.352781 -0.980917 -0.599089 -0.699424  0.397115 -0.021739  17.1   \n",
       "483  0.143403  1.666143  1.531405  0.815776  0.384989 -0.308047  21.8   \n",
       "135 -0.802066 -0.635649  0.183662  1.274928  0.404281  0.623524  18.1   \n",
       "39   0.763123 -0.750738 -0.905127 -0.056612  0.414864 -1.176944  30.8   \n",
       "316  0.095977 -0.635649 -0.599089 -0.010697  0.360515  0.818669  17.8   \n",
       "287  1.674581 -0.405469 -0.663828 -0.837170  0.428865 -0.775257  23.2   \n",
       "398 -1.097513  1.666143  1.531405  0.815776  0.428865  2.565008   5.0   \n",
       "388 -1.049992  1.666143  1.531405  0.815776  0.164504  2.569282  10.2   \n",
       "74   0.216278 -0.520559 -0.045867  0.127049  0.407037 -0.826536  24.1   \n",
       "434 -0.749027  1.666143  1.531405  0.815776 -2.837287  0.368552  11.7   \n",
       "237  0.167044 -0.175290 -0.581433 -0.469848  0.353570 -1.118542  31.5   \n",
       "380 -1.132286  1.666143  1.531405  0.815776  0.428865  0.659134  10.4   \n",
       "111 -0.532448 -0.405469  0.154235 -0.286188  0.414423 -0.345082  22.8   \n",
       "343  0.920575 -0.520559 -0.210656 -0.378018  0.428865 -0.769560  23.9   \n",
       "414 -1.017313  1.666143  1.531405  0.815776 -2.973547  3.475213   7.0   \n",
       "206  0.265463 -0.635649 -0.757993  0.081134  0.406486 -0.229704  24.4   \n",
       "497 -0.430081 -0.405469 -0.087064  0.356625  0.428865  0.216139  18.3   \n",
       "152 -1.040146 -0.520559 -0.016440 -1.709558 -0.162255 -0.065896  15.3   \n",
       "165 -0.719915 -0.520559 -0.016440 -1.709558 -1.299075 -0.394937  25.0   \n",
       "350  2.376167 -0.980917 -0.416644  0.586200  0.428865 -0.940490  22.9   \n",
       "360 -0.608225  1.666143  1.531405  0.815776  0.182584 -0.682670  25.0   \n",
       "34  -0.017616 -0.635649 -0.581433  1.183098 -1.209227  1.104978  13.5   \n",
       "504 -0.669731 -0.980917 -0.781535  1.183098  0.390831 -0.869269  22.0   \n",
       "36  -0.199280 -0.520559 -0.746223  0.356625  0.215656 -0.167030  20.0   \n",
       "32   0.091886 -0.635649 -0.581433  1.183098 -1.382418  2.154775  13.2   \n",
       "291  0.627839 -0.635649 -0.946324  0.356625  0.428865 -1.285200  37.3   \n",
       "159 -0.966082 -0.520559 -0.016440 -1.709558  0.069585 -0.739647  23.3   \n",
       "128 -0.943915 -0.635649  0.183662  1.274928  0.428865  0.399890  18.0   \n",
       "286  2.517492 -0.980917 -0.969866 -0.102527 -0.180776  0.049482  20.1   \n",
       "435 -0.795406  1.666143  1.531405  0.815776 -2.735644  1.522333  13.4   \n",
       "72   0.708991 -0.635649 -0.593204  0.356625  0.362830 -1.006013  22.8   \n",
       "190  1.276243 -0.520559 -0.045867 -1.479982  0.216979 -1.065839  37.0   \n",
       "188  0.366213 -0.520559 -0.045867 -1.479982  0.273864 -1.142758  29.8   \n",
       "\n",
       "     MEDV_predict  \n",
       "409     15.620402  \n",
       "247     20.404778  \n",
       "399     11.251188  \n",
       "300     29.942523  \n",
       "321     24.774775  \n",
       "31      18.102665  \n",
       "342     21.575208  \n",
       "375     19.375041  \n",
       "379     14.089632  \n",
       "288     27.029747  \n",
       "229     31.950960  \n",
       "129     14.915996  \n",
       "208     22.284157  \n",
       "162     42.044609  \n",
       "428     13.470982  \n",
       "110     19.675269  \n",
       "429     12.252919  \n",
       "314     24.388828  \n",
       "331     20.153791  \n",
       "483     21.304634  \n",
       "135     16.440503  \n",
       "39      30.755921  \n",
       "316     17.415248  \n",
       "287     27.095258  \n",
       "398      7.976684  \n",
       "388      9.732137  \n",
       "74      25.293943  \n",
       "434     14.508443  \n",
       "237     32.344997  \n",
       "380      7.682925  \n",
       "111     24.649605  \n",
       "343     26.238518  \n",
       "414      4.865122  \n",
       "206     22.959860  \n",
       "497     18.927879  \n",
       "152     20.521441  \n",
       "165     23.937962  \n",
       "350     20.754055  \n",
       "360     21.653252  \n",
       "34      14.142180  \n",
       "504     24.807686  \n",
       "36      22.123834  \n",
       "32      11.121791  \n",
       "291     33.606538  \n",
       "159     24.302258  \n",
       "128     17.656444  \n",
       "286     19.347626  \n",
       "435     12.199567  \n",
       "72      25.297267  \n",
       "190     29.909028  \n",
       "188     32.276933  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict Results on test set\n",
    "\n",
    "\n",
    "df_predict = pd.concat([X_test,y_test],axis=1)\n",
    "df_predict['MEDV_predict'] = np.exp(rreg.predict(X_test))\n",
    "\n",
    "df_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forests With the Boston Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X and y variables for Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide it into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a Random Forest on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are its most important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How well does your model perform on your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge:  Try and find at least two improvements to your model to improve test scores.\n",
    "\n",
    "You can try the following:\n",
    " - increasing the number of trees\n",
    " - using a different number of maximum features to sample\n",
    " - using a different number of minimum samples per leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using the Statsmodels Formula\n",
    "\n",
    "Adapt the formula example using your metrics. We will review this implementation in class. Here is a reference to consider. The workflow is the same, but the syntax is a little different. We want to get accustomed to the formula syntax because we will be using them a lot more with regressions. The results should be comparable to scikit-learn's regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, format our data in a DataFrame\n",
    "\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['MEDV'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our new statsmodel.formula handling model\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# You can easily swap these out to test multiple versions/different formulas\n",
    "formulas = {\n",
    "    \"case1\": \"MEDV ~ RM + LSTAT + RAD + TAX + NOX + INDUS + CRIM + ZN - 1\", # - 1 = remove intercept\n",
    "    \"case2\": \"MEDV ~ NOX + RM\",\n",
    "    \"case3\": \"MEDV ~ RAD + TAX\"\n",
    "}\n",
    "\n",
    "model = smf.ols(formula=formulas['case1'], data=df)\n",
    "result = model.fit()\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenge #1:\n",
    "\n",
    "Can you optimize your R2, selecting the best features and using either test-train split or k-folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenge #2:\n",
    "\n",
    "Given a combination of predictors, can you find another response variable that can be accurately predicted through the exploration of different predictors in this data set?\n",
    "\n",
    "_Tip: Check out pairplots, coefficients, and Pearson scores._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out variable relations\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out Pearson scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
