{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 2:  Homework Option III -- Working With Large Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework provides students with the option to get practice opening, sampling, and cleaning data when it has file sizes that might stretch your available RAM, or even exceed it.  \n",
    "\n",
    "**What you'll turn in:**\n",
    "\n",
    "You'll make a pull request with a folder (titled as your name) that contains the following:\n",
    "\n",
    " - for section I, a file called `titanic`, stored in a binary `feather` or `parquet` file format, that reduces the `titanic` dataset to its smallest possible memory amount without tampering any of its values.\n",
    " - for section II, simply turn in this notebook with the questions answered.\n",
    " - for section III, a script called `chunking.py` that, when run, will connect to an s3 bucket, and stream a file through memory, clean it, and output it to a `.csv` file, even though the entire file was never entirely loaded into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section I: Downcasting the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What You'll Learn:** The basics of managing memory within files, and how to use advanced file formats such as the `feather` or `parquet`, to make it easier to maintain persistent data types when a file isn't loaded.\n",
    "\n",
    "**What You'll Turn In:** A file called `titanic`, which contains the memory-reduced form of the file.\n",
    "\n",
    "#### Downcasting\n",
    "\n",
    "`Downcasting` is the task of reducing the memory footprint of different columns in your dataset so they take up less RAM when you load them in.  \n",
    "\n",
    "Most software used for handling data makes use of your available RAM to process its tasks.  If the size of your file neatly fits into the available RAM that your computer has then this is fine.  If it's significantly larger (no laptop is going to have 1TB of RAM for example), then you won't be able to load in the file and work with it.\n",
    "\n",
    "Pandas works this way, and therefore the amount of working RAM you have available to use is going to function as a limit for what file sizes you can work with.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Quick Intro to Data Types in Pandas & Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers come in different flavors in pandas and numpy.  At the simplest level you have integers (whole numbers) and floating point numbers (numbers with decimals).  \n",
    "\n",
    "However, numbers use different sized containers to store their values.  They are as follows:\n",
    "\n",
    " - **64 bit:** Can store values as large as 2<sup>64</sup>, which is 18446744073709551616\n",
    " - **32 bit:** Can store values as large as 2<sup>32</sup>, which is 4294967296\n",
    " - **16 bit:** Good for values up to 2<sup>16</sup>, 65536\n",
    " - **8 bit:**  2<sup>8</sup>, or up to 264\n",
    " \n",
    "**Integers** typically have the above range, while **floats** typically can only go down to 32 bits.\n",
    "\n",
    "You can see the whole range of numeric data types here:  https://docs.scipy.org/doc/numpy/user/basics.types.html\n",
    "\n",
    "The important detail here, is that if a number is encoded as being a 64 bit number, it will *always* use the same amount of memory to store it, even if the value itself is much smaller.  \n",
    "\n",
    "So, if you have a column of 0's and 1's, an 8 bit encoding will work perfectly fine (since the values are less than 2<sup>8</sup>), and a 64 bit encoding will take up 8x as much memory as it needs to.\n",
    "\n",
    "An important detail about how Pandas works is that *all numbers are automatically encoded as 64 bit numbers*.  This is good for making sure values aren't tampered with, but bad for optimizing memory with large files.\n",
    "\n",
    "**Methods Used For Managing Memory In Pandas:**\n",
    "\n",
    " - `df.memory_usage()`, returns the memory usage, in bytes, of whatever is selected.\n",
    " - `pd.Series.astype()`, allows you to change the data type of 1 variable to another.\n",
    " - `df.info()`, returns the data type and memory usage of every column selected\n",
    " - `df.dtypes()/pd.Series.dtype()`, returns the data type of everything selected\n",
    " \n",
    "Let's take a look at how these items work.  Run the following cells to get a quick demonstration of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the titanic dataset here -- use a different url if need be to\n",
    "# load it in\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#df = pd.read_csv('./data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# check the info of your dataset -- notice the 64 bit numbers\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index           128\n",
       "PassengerId    7128\n",
       "Survived       7128\n",
       "Pclass         7128\n",
       "Name           7128\n",
       "Sex            7128\n",
       "Age            7128\n",
       "SibSp          7128\n",
       "Parch          7128\n",
       "Ticket         7128\n",
       "Fare           7128\n",
       "Cabin          7128\n",
       "Embarked       7128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also check the memory usage of each column\n",
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's see what happens when we adjust a columns data type\n",
    "df['Survived'].astype(np.int8).memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, an 8 bit number takes up about 1/8 as much memory as a 64 bit number.  Permanently changing something's data type is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will change the data type of a column to something else\n",
    "df['Survived'] = df['Survived'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index           128\n",
       "PassengerId    7128\n",
       "Survived        891\n",
       "Pclass         7128\n",
       "Name           7128\n",
       "Sex            7128\n",
       "Age            7128\n",
       "SibSp          7128\n",
       "Parch          7128\n",
       "Ticket         7128\n",
       "Fare           7128\n",
       "Cabin          7128\n",
       "Embarked       7128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now we can see its memory footprint is permanently smaller\n",
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice however, that if you make a columns data type *smaller* than what it is, the original values will be tampered.  For example, the Passenger ID column has values as large as 891.....which is more than 2<sup>8</sup>.  Notice what happens when you make the change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        2\n",
       "2        3\n",
       "3        4\n",
       "4        5\n",
       "      ... \n",
       "886    119\n",
       "887    120\n",
       "888    121\n",
       "889    122\n",
       "890    123\n",
       "Name: PassengerId, Length: 891, dtype: int8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the values at the end of the series should be 889, 890, 891, etc\n",
    "df['PassengerId'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly, getting things 'just right' is important.  Notice also the difference between **signed** and **unsigned** data types.  If it's signed, that means they can accept negative values.  \n",
    "\n",
    "So, a datatype of `np.uint8` can accept ranges of 0 - 255, whereas `np.int8` accepts values from -128 to 127."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Data in Pandas and Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text based data in Pandas and Numpy has two different varieties:\n",
    "\n",
    " - **np.object**: this is the default numpy way of treating and handling data.  Pretty close to a python string, and is used to store data that doesn't have any other characteristic (integer, float, bool, etc)\n",
    " - **category**: this is a special data type built specifically for Pandas, to handle text data that has a small number of repeating values.  Like the `sex` column in our dataset.  When appropriately used, it can drastically reduce the memory footprint of text based data.  You can read more about them here:  https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    " \n",
    "See below for a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the Sex column currently uses the same memory as a 64 bit number\n",
    "df['Sex'].memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you change it to a category, it uses about the same amount\n",
    "# as an 8 bit number\n",
    "df['Sex'].astype('category').memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that using categories only has the desired effect when there are repeating values.  Make sure to check the memory footprint before making the change!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section I Task:  Clean Up the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of your homework assignment, your task is to reduce the memory footprint of the titanic dataset as much as possible, while not tampering any values, and export it to a binary file format called `feather`(preferably), or `parquet` which will maintain information about all of its data types when it's loaded back in. \n",
    "\n",
    "You can read more about the feather file format here:  https://blog.rstudio.com/2016/03/29/feather/.\n",
    "\n",
    "The file `titanic` should be turned in inside a folder with your name on it, and it will be inspected to make sure it was downcasted in the most appropriate manner.\n",
    "\n",
    "You can use the `to_feather()`, and `read_feather()` methods to load and export your files.  Or the `to_parquet()` and `read_parquet()` methods, respectively.\n",
    "\n",
    "**Note:** It is best to use feather files if you can.  They are the only file format that can maintain persistent information about the `category` data type after it's been saved.\n",
    "\n",
    "However, you will likely need some additional libraries to get it working.  Often pandas does not work well with `feather` files right out of the box.  The typical library used to work with feather files is, appropriately called......feather.  To install the `feather` library, simply go to Anaconda Prompt/Terminal and type in `conda install feather-format`.\n",
    "\n",
    "The library's homepage can be found here:  https://pypi.org/project/feather-format/\n",
    "\n",
    "This library isn't always well supported, and there's a possibility that you might have an exceedingly difficult time getting it to work.  The purpose of this homework is not to have you go down rabbit holes getting an obscure library to work.  If after 30 minutes - 1 hour you still don't have the file installed, feel free to use the `parquet` file format, which can be a little easier to work with.\n",
    "\n",
    "You can find more about it here:  https://arrow.apache.org/docs/python/parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int16\n",
      "Survived       891 non-null int8\n",
      "Pclass         891 non-null int8\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null category\n",
      "Age            714 non-null float32\n",
      "SibSp          891 non-null int8\n",
      "Parch          891 non-null int8\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null category\n",
      "dtypes: category(2), float32(1), float64(1), int16(1), int8(4), object(3)\n",
      "memory usage: 38.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import feather\n",
    "\n",
    "feather.read_dataframe('titanic1.feather').info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section II: Working With Larger File Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section I was your warmup to get the hang of how to reduce the memory of your data and get it loaded into a more advanced file format that can be reused for other projects.\n",
    "\n",
    "This section will extend what you just did, but add in two additional wrinkles:\n",
    "\n",
    " - The file will be much larger -- approximately 2 million rows\n",
    " - You'll find out what types of data types you should be working with.....*without* reading in the entire file in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine this hypothetical scenario:  there's a 10 gb file that you need to work on, and loading it into pandas is returning a `MemoryError`.  You're almost certain you could reduce it to something much smaller......except you can't even read it in to figure out what to do next.\n",
    "\n",
    "These types of chicken & egg problems are fairly common, and a popular antidote to them is to sample in a portion of your dataframe.  There are two primary ways to do this if you're reading in `.csv` files:\n",
    "\n",
    " - `nrows`   : tells you how many rows to read in from the original file\n",
    " - `skiprows`: tells you which rows to *skip* from the original file\n",
    " \n",
    "For both of these you can just manually read in x number of rows relatively easily, but doing so right from the beginning or end has some problems.  Mainly, many datasets don't have consistent values from beginning to end.  \n",
    "\n",
    "For example, if you have a dataset with 10 years of sales info, it's very possible values being recorded are very different at varying time segments.  Often new columns are added to datasets in the middle of their collection, and all values for all times before that are simply `null`, so just reading in the first 5000 or 10000 might not give you a consistent picture of what to expect.\n",
    "\n",
    "For this reason, it's good to randomly sample in dataframes before you want to read them in entirely.\n",
    "\n",
    "To do this, you can mix `lambda` functions (remember those?) with the `skiprows` argument, which can accept a function as values.\n",
    "\n",
    "Here's the basic idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# random.random() will generate a random value between 0 and 1\n",
    "# so this will return True 50% of the time\n",
    "random.random() > .50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this same logic, we can pass this into a lambda function like so, to read in 30% of the titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use read_csv with a lambda function\n",
    "df = pd.read_csv('./data/titanic.csv', skiprows=lambda x: x > 0 and random.random() > 0.3)\n",
    "# and notice the size of the dataframe that was read in\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, inside the `lambda` function, `x` represents the index value of the row being read in.  `x > 0` is used assuming the first row are headers, and of course `random.random() > 0.3` will return `True` 70% of the time, hence the results that we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that you can read in a very small fraction of a very large file if you want to investigate its most important properties.  \n",
    "\n",
    "Now, this begs the question.....how do you specify the data types you want a column to be before you read it in?  \n",
    "\n",
    "There is a very useful argument in `read_csv` called `dtype` that accepts a dictionary, where you can list column labels as keys, and their corresponding data type as a value.\n",
    "\n",
    "So for example, if we wanted to change the `Embarked` column and the `Survived` column to `category` and `np.int8`, we could do so in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary contains the columns we want to change\n",
    "dtypes = {\n",
    "    # column label: new data type\n",
    "    'Embarked': 'category',\n",
    "    'Survived': np.int8\n",
    "}\n",
    "\n",
    "# and now we'll re-read in the .csv file, and use the dtypes dict\n",
    "df = pd.read_csv('../data/titanic.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we can see that the data type of the columns are in fact changed\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section II Task: Sample, Clean Up, And Read in the taxi.csv file\n",
    "\n",
    "This portion of section II contains the task that you will be graded on.  You can simply answer the prompts inside this notebook and turn it in.  No additional files are necessary.  The file is located in an S3 bucket at this location:  `https://dat-data.s3.amazonaws.com/taxi.csv`\n",
    "\n",
    "It records information about every taxi ride given by a particular company for approximately 1 year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part I:** Randomly sample in 10% the taxi.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "df = pd.read_csv('https://dat-data.s3.amazonaws.com/taxi.csv', skiprows=lambda x: x > 0 and random.random() > 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part II:** Go ahead and do the appropriate exploratory data analysis to figure out the most appropriate data type for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index               128\n",
       "TRIP_ID         1369240\n",
       "CALL_TYPE       1369240\n",
       "ORIGIN_CALL     1369240\n",
       "ORIGIN_STAND    1369240\n",
       "TAXI_ID         1369240\n",
       "TIMESTAMP       1369240\n",
       "DAY_TYPE        1369240\n",
       "MISSING_DATA     171155\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer here\n",
    "df.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part III**: Create a dictionary that contains the key/value pairs for each column that needs to be changed to a different data type, and then read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "dtypes = {\n",
    "    'CALL_TYPE': 'category',\n",
    "    'DAY_TYPE': 'category',\n",
    "    'TIMESTAMP': np.int32,\n",
    "    'TAXI_ID': np.int32,\n",
    "    'ORIGIN_STAND': np.float32,\n",
    "    'ORIGIN_CALL': np.float32,\n",
    "}\n",
    "\n",
    "# and now we'll re-read in the .csv file, and use the dtypes dict\n",
    "df = pd.read_csv('https://dat-data.s3.amazonaws.com/taxi.csv', dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part IV:** Confirm that each column has the appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1710670 entries, 0 to 1710669\n",
      "Data columns (total 8 columns):\n",
      "TRIP_ID         int64\n",
      "CALL_TYPE       category\n",
      "ORIGIN_CALL     float32\n",
      "ORIGIN_STAND    float32\n",
      "TAXI_ID         int32\n",
      "TIMESTAMP       int32\n",
      "DAY_TYPE        category\n",
      "MISSING_DATA    bool\n",
      "dtypes: bool(1), category(2), float32(2), int32(2), int64(1)\n",
      "memory usage: 44.0 MB\n"
     ]
    }
   ],
   "source": [
    "# your answer here\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section III (Optional): File Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File streaming is a way to get around memory limitations in pandas.  When you stream in a file, you spoon feed a portion of it into memory, and when you're finished, load in the next portion, and so on until there's nothing left. \n",
    "\n",
    "It's less convenient than regular file I/O, but it removes any sort of memory limit you might face when working with a file because it's never loaded at the same time. \n",
    "\n",
    "In this section of the homework assignment, you'll be tasked with performing basic cleaning operations on a file......without ever having to load it into memory.  This means what you accomplish in this section you could perform on a file of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Introduction to File Streams\n",
    "\n",
    "When it's available, you can specify a file stream by using the `chunksize` argument, which specifies that you only read in so many lines at a time.\n",
    "\n",
    "Notice how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.io.parsers.TextFileReader"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when we read in the csv file, we'll set chunksize to 200\n",
    "df = pd.read_csv('./data/titanic.csv', chunksize=250)\n",
    "\n",
    "# notice that df is NOT a df.....it's a file stream\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Minahan, Dr. William Edward</td>\n",
       "      <td>male</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19928</td>\n",
       "      <td>90.0000</td>\n",
       "      <td>C78</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Lindahl, Miss. Agda Thorilda Viktoria</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>347071</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hamalainen, Mrs. William (Anna)</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>250649</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Beckwith, Mr. Richard Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11751</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>D35</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Carter, Rev. Ernest Courtenay</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>244252</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "245          246         0       1   \n",
       "246          247         0       3   \n",
       "247          248         1       2   \n",
       "248          249         1       1   \n",
       "249          250         0       2   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "245                        Minahan, Dr. William Edward    male  44.0      2   \n",
       "246              Lindahl, Miss. Agda Thorilda Viktoria  female  25.0      0   \n",
       "247                    Hamalainen, Mrs. William (Anna)  female  24.0      0   \n",
       "248                      Beckwith, Mr. Richard Leonard    male  37.0      1   \n",
       "249                      Carter, Rev. Ernest Courtenay    male  54.0      1   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "245      0             19928  90.0000   C78        Q  \n",
       "246      0            347071   7.7750   NaN        S  \n",
       "247      2            250649  14.5000   NaN        S  \n",
       "248      1             11751  52.5542   D35        S  \n",
       "249      0            244252  26.0000   NaN        S  \n",
       "\n",
       "[250 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now if we want, we can 'chunk' in 250 rows at a time\n",
    "df.get_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Reed, Mr. James George</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>362316</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Strom, Mrs. Wilhelm (Elna Matilda Persson)</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>347054</td>\n",
       "      <td>10.4625</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Stead, Mr. William Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113514</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C87</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Lobb, Mr. William Arthur</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 3336</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rosblom, Mrs. Viktor (Helena Wilhelmina)</td>\n",
       "      <td>female</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>370129</td>\n",
       "      <td>20.2125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Yousseff, Mr. Gerious</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2627</td>\n",
       "      <td>14.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>497</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Eustis, Miss. Elizabeth Mussey</td>\n",
       "      <td>female</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36947</td>\n",
       "      <td>78.2667</td>\n",
       "      <td>D20</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>498</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Shellard, Mr. Frederick William</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A. 6212</td>\n",
       "      <td>15.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Svensson, Mr. Olof</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350035</td>\n",
       "      <td>7.7958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "250          251         0       3   \n",
       "251          252         0       3   \n",
       "252          253         0       1   \n",
       "253          254         0       3   \n",
       "254          255         0       3   \n",
       "..           ...       ...     ...   \n",
       "495          496         0       3   \n",
       "496          497         1       1   \n",
       "497          498         0       3   \n",
       "498          499         0       1   \n",
       "499          500         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "250                           Reed, Mr. James George    male   NaN      0   \n",
       "251       Strom, Mrs. Wilhelm (Elna Matilda Persson)  female  29.0      1   \n",
       "252                        Stead, Mr. William Thomas    male  62.0      0   \n",
       "253                         Lobb, Mr. William Arthur    male  30.0      1   \n",
       "254         Rosblom, Mrs. Viktor (Helena Wilhelmina)  female  41.0      0   \n",
       "..                                               ...     ...   ...    ...   \n",
       "495                            Yousseff, Mr. Gerious    male   NaN      0   \n",
       "496                   Eustis, Miss. Elizabeth Mussey  female  54.0      1   \n",
       "497                  Shellard, Mr. Frederick William    male   NaN      0   \n",
       "498  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female  25.0      1   \n",
       "499                               Svensson, Mr. Olof    male  24.0      0   \n",
       "\n",
       "     Parch     Ticket      Fare    Cabin Embarked  \n",
       "250      0     362316    7.2500      NaN        S  \n",
       "251      1     347054   10.4625       G6        S  \n",
       "252      0     113514   26.5500      C87        S  \n",
       "253      0  A/5. 3336   16.1000      NaN        S  \n",
       "254      2     370129   20.2125      NaN        S  \n",
       "..     ...        ...       ...      ...      ...  \n",
       "495      0       2627   14.4583      NaN        C  \n",
       "496      0      36947   78.2667      D20        C  \n",
       "497      0  C.A. 6212   15.1000      NaN        S  \n",
       "498      2     113781  151.5500  C22 C26        S  \n",
       "499      0     350035    7.7958      NaN        S  \n",
       "\n",
       "[250 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and this would be the next 250 rows\n",
    "df.get_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this basic syntax, you can lazily read in bits and pieces of a file at a time, without having to worry about being able to fit the entire file into RAM at once.  This is useful for doing some basic exploratory data analysis, and peek into a file, even if it's unreasonably large.\n",
    "\n",
    "However, what if you wanted some way to go through the *entire* file, and make bulk changes to it?  \n",
    "\n",
    "You can do that as well, by looping through the file stream. Here's a simple example, that prints off the total memory footprint for each chunk being read in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19328\n",
      "19332\n",
      "19332\n",
      "19332\n",
      "8868\n"
     ]
    }
   ],
   "source": [
    "# this basically reads: for every chunk in the file stream\n",
    "for chunk in pd.read_csv('./data/titanic.csv', chunksize=200):\n",
    "    # do this to each chunk\n",
    "    print(chunk.memory_usage().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what we're doing here.  `chunk` is the portion of the file that you're reading in and operating on.  `pd.read_csv(.....)` in this case is the iterator that you're looping over, not the entire file itself.\n",
    "\n",
    "What's a little difficult to get used to is that `chunk` isn't some stand alone file that you play with once you've loaded it in.  It's just a portion of a loop that you pass some commands to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this section of the homework, you'll need to write functions that allow you to stream in data, and perform specific operations on your data set, without having it entirely loaded into memory, or being able to see it.\n",
    "\n",
    "These functions should be written in a file called `chunking.py`, and each of the following two functions should be able to be called from an IDE to observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 1**\n",
    "\n",
    "**Name:** `probe_df`\n",
    "\n",
    "**Arguments:** \n",
    " - `file_path`, str; required,  Location of file to read in.\n",
    " - `chunksize`, int, required, default value is 1000.  Size of the chunk to use when streaming in the file.\n",
    " \n",
    "**Returns:** a dictionary encoded in the following way: \n",
    " - each key is the name of a column within your dataset\n",
    " - the value for each key is another dictionary with the following key/value pairs:\n",
    "   - `null values`: number of null values for that column\n",
    "   - `dtype`: data type for that column\n",
    "   - `avg_val`: average value for that column ( if numeric, otherwise don't include )\n",
    "   \n",
    "**Note:** The `chunksize` argument can be used with a variety of file types, but you can just assume that you'll be reading a csv file, and nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function II**\n",
    "\n",
    "**Name:** `write_df`\n",
    "\n",
    "**Arguments:**\n",
    "\n",
    " - `file_path_read`   : str, required; location of file to read in.\n",
    " - `file_path_write`  : str, required; location of the file to write the new file out to\n",
    " - `chunksize`, int, required, default value is 1000.  Size of the chunk to use when streaming in the file.\n",
    " - `missing_vals`: dict, optional; accepts a dictionary as an argument with key/value pairs that list the column in the dataset(key) as well as the value to fill missing values with for that column(value).  The values in this dictionary will be used to fill missing values in the file at the location in `file_path_read`.\n",
    " \n",
    "**Returns:**\n",
    "\n",
    "This function will **not** return a value in the terminal.  What it **will** do is write a new file to the location specified in `file_path_write`.  \n",
    "\n",
    "So, for example if you call `write_df('file/path/to/stream', 'file/path/to/write')`, a new file will appear in the location at `file/path/to/write` as the function is being called.\n",
    "\n",
    "The big idea behind this file is that you'll be able to do data cleaning operations on a file you've never actually seen in your terminal before.\n",
    "\n",
    "**Hint:** Pandas has a `to_csv()` method, with the option of appending lines to the end of it -- this is good to use when looping through the file stream.\n",
    "\n",
    "**Note:** We will check to see that column headers are not added multiple times!\n",
    "\n",
    "**To Test:** We will first call `probe_df` on the `taxi` dataset in its original location to get a dictionary with its missing values.  We will then use that value as a basis for the `missing_vals` argument in the `write_df` function, and use those to fill in its missing values.\n",
    "\n",
    "If both of these functions work as intended, they will allow us to fill in the dataset's missing values without having looked at it, and you will receive full marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
